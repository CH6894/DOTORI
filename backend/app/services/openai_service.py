from typing import List, Tuple
import uuid
import logging
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.memory import ConversationSummaryMemory
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

from app.config.settings import settings
from app.models.chat import ChatMessage, MessageRole

logger = logging.getLogger(__name__)

class OpenAIService:
    def __init__(self):
        """
        RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        """
        try:
          
            self.llm = ChatOpenAI(
                model="gpt-3.5-turbo",
                openai_api_key=settings.OPENAI_API_KEY,
                temperature=0.2,
            )
            
            # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (pdfë¥¼ ë³´ê¸° ìœ„í•´ì„œ ê¼­ í•„ìš”í•œ ì½”ë“œ)
            # ë¬¸ìë¥¼ ìˆ«ìë¡œ(ë°±í„°í™”) ëŒë¦¬ëŠ” ì‘ì—… (=ì„ë² ë”©)
            self.embeddings = OpenAIEmbeddings(
                openai_api_key=settings.OPENAI_API_KEY
            )
            
            # ì„¸ì…˜ë³„ ë©”ëª¨ë¦¬ ì €ì¥ì†Œ
            self.session_memories = {}
            
            # PDF ë¬¸ì„œ ë¡œë”© ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            self._setup_document_retrieval()
            
            logger.info("âœ… RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _setup_document_retrieval(self):
        """ë¬¸ì„œ ë¡œë”© ë° ë²¡í„°ìŠ¤í† ì–´ ì„¤ì •"""
        try:
            # PDF ê²½ë¡œ
            pdf_path = "C:\\Users\\smhrd\\Documents\\figure_dummy_200.pdf"
            
            # PDF ë¡œë”©
            logger.info(f"ğŸ“„ PDF ë¬¸ì„œ ë¡œë”© ì¤‘: {pdf_path}")
            loader = PyPDFLoader(pdf_path)
            documents = loader.load()
            
            # í…ìŠ¤íŠ¸ ë¶„í• 
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=50,
                separators=["\n\n", "\n", ".", " ", ""]
            )
            texts = text_splitter.split_documents(documents)
            
            # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            logger.info("ğŸ” ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
            self.vectorstore = FAISS.from_documents(
                documents=texts,
                embedding=self.embeddings
            )
            self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": 5})
            
            # í”¼ê·œì–´ ì „ë¬¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
            prompt_template = """ë‹¹ì‹ ì€ í”¼ê·œì–´ ì „ë¬¸ ì‡¼í•‘ëª°ì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ í”¼ê·œì–´ ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.

í”¼ê·œì–´ ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ë‹µë³€ ì§€ì¹¨:
- í”¼ê·œì–´ ê´€ë ¨ ì§ˆë¬¸ì´ë©´ ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ë¥¼ ì°¸ì¡°í•´ì„œ êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”
- ê°€ê²©, ì¬ê³ , í¬ê·€ë„, ìƒíƒœ ë“±ì˜ ì •ë³´ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”  
- ì¼ë°˜ì ì¸ ëŒ€í™”ë„ í”¼ê·œì–´ ì‡¼í•‘ëª° ì–´ì‹œìŠ¤í„´íŠ¸ë¡œì„œ ì¹œê·¼í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”
- ë°ì´í„°ì— ì—†ëŠ” ì •ë³´ëŠ” "ì£„ì†¡í•˜ì§€ë§Œ í•´ë‹¹ ì •ë³´ëŠ” í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ì•ˆë‚´í•´ì£¼ì„¸ìš”

ë‹µë³€:"""

            PROMPT = PromptTemplate(
                template=prompt_template, 
                input_variables=["context", "question"]
            )
            
            # RetrievalQA ì²´ì¸ ìƒì„± (ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•´ RAG ì‚¬ìš©)
            self.chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=self.retriever,
                return_source_documents=True,
                chain_type_kwargs={"prompt": PROMPT}
            )
            
            logger.info(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {len(texts)}ê°œ ì²­í¬ ìƒì„±")
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _get_or_create_memory(self, session_id: str):
        """ì„¸ì…˜ë³„ ë©”ëª¨ë¦¬ ê´€ë¦¬ ê¸°ì–µí•˜ê¸°"""
        if session_id not in self.session_memories:
            self.session_memories[session_id] = ConversationSummaryMemory(
                llm=self.llm,
                max_token_limit=80,
                memory_key="chat_history",
                return_messages=True,
            )
            logger.info(f"ğŸ§  ìƒˆ ë©”ëª¨ë¦¬ ìƒì„±: {session_id}")
        
        return self.session_memories[session_id]
    
    async def generate_response(
        self, 
        user_message: str, 
        session_id: str = None,
        conversation_history: List[ChatMessage] = None
    ) -> Tuple[str, str]:
        """RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„± (ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•´ PDF ì°¸ì¡°)"""
        try:
            # ì„¸ì…˜ ID ì²˜ë¦¬
            if not session_id:
                session_id = str(uuid.uuid4())
            
            # ì„¸ì…˜ë³„ ë©”ëª¨ë¦¬ ê°€ì ¸ì˜¤ê¸°
            memory = self._get_or_create_memory(session_id)
            
            logger.info(f"ğŸ¤– RAG ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: [{session_id}] {user_message[:50]}...")
            
            # RetrievalQA ì²´ì¸ìœ¼ë¡œ ë‹µë³€ ìƒì„± (í•­ìƒ PDF ë°ì´í„° ì°¸ì¡°)
            result = self.chain.invoke({"query": user_message})
            ai_response = result["result"]
            
            # ë©”ëª¨ë¦¬ì— ëŒ€í™” ì €ì¥
            memory.save_context(
                {"input": user_message},
                {"output": ai_response}
            )
            
            logger.info(f"âœ… RAG ì‘ë‹µ ìƒì„± ì™„ë£Œ: [{session_id}]")
            logger.info(f"ğŸ“š ì°¸ì¡°ëœ ë¬¸ì„œ ìˆ˜: {len(result.get('source_documents', []))}")
            
            return ai_response, session_id
            
        except Exception as e:
            logger.error(f"âŒ RAG ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"ì§ˆë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    # generate_general_response ë©”ì„œë“œ ì œê±° - ëª¨ë“  ì‘ë‹µì´ RAGë¥¼ ì‚¬ìš©í•˜ë„ë¡ í•¨
    
    async def health_check(self) -> bool:
        """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        try:
            self.chain.invoke({"query": "í…ŒìŠ¤íŠ¸"})
            return True
        except Exception as e:
            logger.error(f"âŒ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
            return False

# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
openai_service = OpenAIService()